dino:
  head_n_prototypes: 131072
  head_bottleneck_dim: 384
  koleo_loss_weight: 0.1
ibot:
  separate_head: true
  head_n_prototypes: 131072
train:
  batch_size_per_gpu: 32 # yolov8-m
  # batch_size_per_gpu: 32 # yolov8-x
  dataset_path: ImageNet22k
  centering: sinkhorn_knopp
  # OFFICIAL_EPOCH_LENGTH: 5000 # (number of images) Tiip, batch_size 64
  # OFFICIAL_EPOCH_LENGTH: 20000 # (number of images) Imagenet, batch_size 64
  OFFICIAL_EPOCH_LENGTH: 10000
student:
  arch: vit_large
  patch_size: 32
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 4
teacher:
  arch: 'dinov2_vits14'
  embed_dim: 384
  momentum_teacher: 0.994
optim:
  # from ssl defualt config
  epochs: 100
  weight_decay: 0.04
  # weight_decay_end: 0.4
  # base_lr: 0.004  # learning rate for a batch size of 1024
  lr: 0.  # will be set after applying scaling rule
  warmup_epochs: 0
  min_lr: 1.0e-07
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  # layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
  #
  weight_decay_end: 0.2
  base_lr: 2.0e-05  # learning rate for a batch size of 1024
  layerwise_decay: 1.0
crops:
  local_crops_size: 98
  global_crops_size: 448
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
distill:
  True
yolo_cfg:
  patch_size: 32
  yolo_yaml_path: /home/julian/work/dinov2/ultralytics/ultralytics/cfg/models/v8/yolov8m-ssl.yaml

# if distill is True, then teacher arch, and embed_dim are required